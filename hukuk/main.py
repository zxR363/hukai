import sys

# Ã‡Ä±ktÄ± karakter hatasÄ± olmasÄ±n diye
sys.stdout.reconfigure(encoding='utf-8')

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_classic.chains import create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# --- %100 LOKAL KÃœTÃœPHANELER ---
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama  # <-- Yeni oyuncumuz bu


def create_local_cv_bot():
    print("--- 1. PDF YÃ¼kleniyor... ---")
    try:
        loader = PyPDFLoader("Yusuf_Ustuntepe_CV_tr.pdf")
        docs = loader.load()
    except Exception as e:
        print(f"HATA: PDF bulunamadÄ±. Detay: {e}")
        return None

    print("--- 2. Metin ParÃ§alanÄ±yor... ---")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = text_splitter.split_documents(docs)

    print("--- 3. VektÃ¶r VeritabanÄ± (Local CPU)... ---")
    # Embedding: Metni sayÄ±ya Ã§eviren kÄ±sÄ±m (HuggingFace - Local)
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

    vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    print("--- 4. LLM (Ollama) BaÄŸlanÄ±yor... ---")
    # LLM: CevabÄ± veren kÄ±sÄ±m (Ollama - Local)
    # 'llama3.2' modelini az Ã¶nce terminalden indirdik.
    llm = ChatOllama(model="llama3.2", temperature=0)

    system_prompt = (
        "Sen teknik bir iÅŸe alÄ±m asistanÄ±sÄ±n. "
        "AÅŸaÄŸÄ±daki CV iÃ§eriÄŸine dayanarak sorularÄ± TÃ¼rkÃ§e cevapla. "
        "BilmediÄŸin bir ÅŸey sorulursa uydurma. "
        "\n\n"
        "{context}"
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    # Zinciri oluÅŸturuyoruz
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    return rag_chain


if __name__ == "__main__":
    print("\nðŸ–¥ï¸ SÄ°STEM BAÅžLATILIYOR (Lokal Mod)...")
    bot = create_local_cv_bot()

    if bot:
        print("\nâœ… OLLAMA BOT HAZIR! (Ä°nternet baÄŸlantÄ±sÄ± gerekmez)")
        print("Ã‡Ä±kmak iÃ§in 'q' yazÄ±n.\n")

        while True:
            try:
                soru = input("Soru Sor: ")
                if soru.lower() == 'q':
                    break

                print("â³ DÃ¼ÅŸÃ¼nÃ¼yor (Ä°ÅŸlemcinize baÄŸlÄ± olarak biraz sÃ¼rebilir)...")
                response = bot.invoke({"input": soru})
                print(f"\nCEVAP: {response['answer']}")
                print("-" * 50)
            except Exception as e:
                print(f"Hata: {e}")
                print("Ä°PUCU: 'ollama' uygulamasÄ±nÄ±n arka planda aÃ§Ä±k olduÄŸundan emin misin?")