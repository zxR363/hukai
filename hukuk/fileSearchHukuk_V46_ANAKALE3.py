import sys
import os
import re
import uuid
import time
from multiprocessing import Pool, cpu_count

# --------------------------------------------------
# ğŸ“¦ IMPORTLAR
# --------------------------------------------------
import fitz  # PyMuPDF (V44 Ä°Ã‡Ä°N GEREKLÄ° - PDF SONU OKUMA)
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import ChatOllama, OllamaEmbeddings
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from fpdf import FPDF
from fpdf.enums import XPos, YPos
from langchain_community.document_loaders import PyMuPDFLoader  # Loader dÃ¼zeltmesi

# UTF-8 AyarÄ±
sys.stdout.reconfigure(encoding="utf-8")

# ================== AYARLAR (V45 Ä°LE AYNI) ==================
SOURCES = {
    "mevzuat": {
        "folder": "mevzuatlar",
        "collection": "legal_statutes_v43",
        "desc": "MEVZUAT"
    },
    "emsal": {
        "folder": "belgeler",
        "collection": "legal_precedents_v43",
        "desc": "EMSAL KARAR"
    }
}

QDRANT_PATH = "qdrant_db_master"
EMBEDDING_MODEL = "nomic-embed-text"

SEARCH_LIMIT_PER_SOURCE = 30
SCORE_THRESHOLD = 0.40
LLM_RERANK_LIMIT = 10


# ==================================================
# 1ï¸âƒ£ MODÃœL: PDF SONU OKUYUCU (V44 Ä°LE AYNI)
# ==================================================
def extract_pdf_conclusion(file_path, char_limit=2500):
    """
    VektÃ¶r aramasÄ± dosyanÄ±n baÅŸÄ±nÄ± bulsa bile, bu fonksiyon
    fiziksel dosyaya gidip PDF'in SON SAYFALARINI okur.
    Ã‡Ã¼nkÃ¼ YargÄ±tay kararlarÄ± genelde en sonda 'HÃœKÃœM:...' der.
    """
    try:
        if not os.path.exists(file_path):
            return "[Dosya bulunamadÄ±, fiziksel okuma yapÄ±lamadÄ±.]"

        doc = fitz.open(file_path)
        total_pages = len(doc)
        text = ""

        # Son 2 sayfayÄ± hedefle (Karar ve SonuÃ§ genelde buradadÄ±r)
        start_page = max(0, total_pages - 2)

        for i in range(start_page, total_pages):
            text += doc[i].get_text()

        doc.close()

        # Metnin son X karakterini temizleyip dÃ¶ndÃ¼r
        return text[-char_limit:]
    except Exception as e:
        return f"[Karar kÄ±smÄ± okunamadÄ±: {e}]"


# ==================================================
# 2ï¸âƒ£ MEVCUT ARAÃ‡LAR (V45 Ä°LE AYNI)
# ==================================================
def worker_embed_batch(args):
    texts, model_name = args
    embedder = OllamaEmbeddings(model=model_name)
    return embedder.embed_documents(texts)


def clean_text(text):
    text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def generate_expanded_queries(llm, story, topic):
    print("   â†³ ğŸ§  Sorgu GeniÅŸletiliyor...")
    prompt = f"""GÃ–REV: Hukuki olayÄ± analiz et.
OLAY: "{story}"
ODAK: {topic}
Arama motoru iÃ§in 3 farklÄ± bakÄ±ÅŸ aÃ§Ä±sÄ±yla (Hakim, Avukat, Mevzuat) 3 kÄ±sa cÃ¼mle yaz. BaÅŸlÄ±k koyma."""
    try:
        res = llm.invoke(prompt).content
        return [line.strip() for line in res.splitlines() if len(line) > 10][:3]
    except:
        return [story]


def check_relevance_judge_smart(llm, user_query, user_filter, negative_keywords, document_text, source_name):
    found_negative = None
    if negative_keywords:
        doc_lower = document_text.lower()
        for bad in negative_keywords:
            pattern = re.compile(rf"\b{re.escape(bad)}\b")
            if pattern.search(doc_lower):
                found_negative = bad
                break

    if found_negative:
        prompt = f"""
SEN HUKUK EDÄ°TÃ–RÃœSÃœN.
Sorgu: "{user_query}" ({user_filter}).
Belgede yasaklÄ± "{found_negative}" kelimesi geÃ§iyor.
Belge: "{document_text[:600]}..."
Bu kelime konuyu tamamen saptÄ±rÄ±yor mu (RED)? Yoksa baÄŸlam uygun mu (KABUL)?
CEVAP: [RED] veya [KABUL] ve sebebi.
"""
        res = llm.invoke(prompt).content.strip()
        if "RED" in res:
            return False, f"â›” YASAKLI KELÄ°ME ({found_negative}): {res}"

    scope = f"Odak: {user_filter}" if user_filter else "Genel Hukuk"
    prompt_gen = f"""
SEN HUKUKÃ‡USUN.
Sorgu: "{user_query}"
BaÄŸlam: {scope}
Belge: "{document_text[:600]}..."
Bu belge bu konuya delil olabilir mi?
CEVAP: [EVET] veya [HAYIR] ve sebebi.
"""
    res = llm.invoke(prompt_gen).content.strip()
    return "EVET" in res.upper(), res


# ==================================================
# 3ï¸âƒ£ INDEXING ENGINE (V45 Ä°LE AYNI)
# ==================================================
def run_indexing_v44():
    client = QdrantClient(path=QDRANT_PATH)

    for key, config in SOURCES.items():
        collection_name = config["collection"]
        folder_path = config["folder"]

        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
            print(f"âš ï¸ '{folder_path}' klasÃ¶rÃ¼ oluÅŸturuldu.")
            continue

        if not client.collection_exists(collection_name):
            print(f"âš™ï¸ '{collection_name}' ({config['desc']}) kutusu oluÅŸturuluyor...")
            client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(size=768, distance=Distance.COSINE)
            )

        indexed_files = set()
        offset = None
        while True:
            points, offset = client.scroll(collection_name, offset=offset, limit=100, with_payload=True,
                                           with_vectors=False)
            for p in points:
                if 'source' in p.payload: indexed_files.add(p.payload['source'])
            if offset is None: break

        files_on_disk = [f for f in os.listdir(folder_path) if f.endswith('.pdf')]
        new_files = [f for f in files_on_disk if f not in indexed_files]

        if not new_files:
            print(f"âœ… {config['desc']} gÃ¼ncel.")
            continue

        print(f"â™»ï¸ {config['desc']} iÃ§in {len(new_files)} yeni dosya iÅŸleniyor...")

        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        all_texts = []
        all_metadatas = []

        for filename in new_files:
            try:
                loader = PyMuPDFLoader(os.path.join(folder_path, filename))
                docs = loader.load()
                chunks = text_splitter.split_documents(docs)
                for c in chunks:
                    clean_content = clean_text(c.page_content)
                    all_texts.append(clean_content)
                    all_metadatas.append({
                        "source": filename,
                        "type": config['desc'],
                        "page": c.metadata.get("page", 0) + 1
                    })
                print(f"   ğŸ“„ Okundu: {filename}")
            except Exception as e:
                print(f"   âš ï¸ Hata: {filename} - {e}")

        if not all_texts: continue

        print(f"   ğŸš€ VektÃ¶rleÅŸtiriliyor ({len(all_texts)} parÃ§a)...")
        num_cores = cpu_count()
        batch_size = (len(all_texts) // num_cores) + 1
        batches = []
        for i in range(0, len(all_texts), batch_size):
            batches.append((all_texts[i:i + batch_size], EMBEDDING_MODEL))

        all_vectors = []
        with Pool(processes=num_cores) as pool:
            results = pool.map(worker_embed_batch, batches)
            for res in results: all_vectors.extend(res)

        print(f"   ğŸ’¾ {collection_name} kutusuna yazÄ±lÄ±yor...")
        points = []
        for i, (vec, meta, txt) in enumerate(zip(all_vectors, all_metadatas, all_texts)):
            payload = {"page_content": txt, "source": meta["source"], "page": meta["page"], "type": meta["type"]}
            point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, txt + meta["source"] + collection_name))
            points.append(PointStruct(id=point_id, vector=vec, payload=payload))

        batch_size_upload = 64
        for i in range(0, len(points), batch_size_upload):
            client.upsert(collection_name, points[i:i + batch_size_upload])

    print("âœ… TÃ¼m indeksleme tamamlandÄ±.")
    return True


# ==================================================
# 4ï¸âƒ£ PDF REPORT
# ==================================================
class LegalReport(FPDF):
    def header(self):
        self.set_font('helvetica', 'B', 15)
        self.cell(0, 10, 'HUKUKI ANALIZ RAPORU', new_x=XPos.LMARGIN, new_y=YPos.NEXT, align='C');
        self.ln(5)

    def footer(self):
        self.set_y(-15);
        self.set_font('helvetica', 'I', 8);
        self.cell(0, 10, f'Sayfa {self.page_no()}', align='C')


def create_pdf_report(user_story, valid_docs, advice_text, filename="Hukuki_Rapor_V46.pdf"):
    pdf = LegalReport();
    pdf.add_page();
    pdf.set_font("helvetica", size=11)

    def clean(t):
        if not t: return ""
        tr = {'ÄŸ': 'g', 'Ã¼': 'u', 'ÅŸ': 's', 'Ä±': 'i', 'Ã¶': 'o', 'Ã§': 'c', 'Ä': 'G', 'Ãœ': 'U', 'Å': 'S', 'Ä°': 'I',
              'Ã–': 'O', 'Ã‡': 'C'}
        for k, v in tr.items(): t = t.replace(k, v)
        return t.encode('latin-1', 'replace').decode('latin-1')

    pdf.set_font(style='B', size=12);
    pdf.cell(0, 10, clean("1. OLAY:"), new_x=XPos.LMARGIN, new_y=YPos.NEXT)
    pdf.set_font(style='', size=10);
    pdf.multi_cell(w=pdf.epw, h=6, text=clean(user_story));
    pdf.ln(5)

    pdf.set_font(style='B', size=12);
    pdf.cell(0, 10, clean("2. KULLANILAN KAYNAKLAR:"), new_x=XPos.LMARGIN, new_y=YPos.NEXT)
    for doc in valid_docs:
        pdf.set_font(style='B', size=9)
        source_title = f"[{doc['type']}] {doc['source']} (Sf. {doc['page']})"
        pdf.cell(0, 6, clean(source_title), new_x=XPos.LMARGIN, new_y=YPos.NEXT)
        pdf.set_font(style='I', size=8);
        pdf.multi_cell(w=pdf.epw, h=4, text=clean(f"SEBEP: {doc['reason']}"));
        pdf.ln(2)

    pdf.add_page();
    pdf.set_font(style='B', size=12);
    pdf.cell(0, 10, clean("3. HUKUKI GORUS:"), new_x=XPos.LMARGIN, new_y=YPos.NEXT)
    pdf.set_font(style='', size=10);
    pdf.multi_cell(w=pdf.epw, h=6, text=clean(advice_text))
    try:
        pdf.output(filename); print(f"\nğŸ“„ Rapor HazÄ±r: {filename}")
    except:
        pass


# ==================================================
# 5ï¸âƒ£ ANA MOTOR (V46: FULL DISCLOSURE MODE)
# ==================================================
def main():
    print("ğŸš€ LEGAL SUITE V46 (Full Disclosure: All Evidence Mode)...")

    if not run_indexing_v44():
        sys.exit()

    llm = ChatOllama(model="qwen2.5", temperature=0.1)
    dense_embedder = OllamaEmbeddings(model=EMBEDDING_MODEL)
    client = QdrantClient(path=QDRANT_PATH)

    print("\nâœ… SÄ°STEM HAZIR. (Ã‡Ä±kÄ±ÅŸ: 'q')")

    while True:
        print("-" * 60)
        story = input("ğŸ“ Olay: ")
        if story.lower() == "q": break
        topic = input("ğŸ¯ Odak: ")
        neg_input = input("ğŸš« YasaklÄ±: ")
        negatives = [w.strip().lower() for w in neg_input.split(",")] if neg_input else []

        expanded = generate_expanded_queries(llm, story, topic)
        full_query = f"{story} {topic} " + " ".join(expanded)
        print(f"   âœ“ Sorgu: {len(full_query)} karakter")

        print("\nğŸ” Belgeler TaranÄ±yor (Dual Search)...")
        query_vector = dense_embedder.embed_query(full_query)
        all_candidates = []

        for key, config in SOURCES.items():
            results = client.query_points(
                collection_name=config["collection"],
                query=query_vector,
                limit=40
            ).points
            for hit in results:
                if 'type' not in hit.payload: hit.payload['type'] = config['desc']
                all_candidates.append(hit)

        unique_docs = {}
        for hit in all_candidates:
            if hit.score < SCORE_THRESHOLD: continue
            key = f"{hit.payload['source']}_{hit.payload['page']}"
            if key not in unique_docs or hit.score > unique_docs[key].score:
                unique_docs[key] = hit

        candidates = sorted(unique_docs.values(), key=lambda x: x.score, reverse=True)[:LLM_RERANK_LIMIT]
        if not candidates: print("ğŸ”´ Skor eÅŸiÄŸini geÃ§en belge bulunamadÄ±."); continue

        print("\nâš–ï¸  AkÄ±llÄ± YargÄ±Ã§ DeÄŸerlendiriyor:")
        valid_docs = []

        for hit in candidates:
            doc_text = hit.payload['page_content']
            source = hit.payload['source']
            page = hit.payload['page']
            type_desc = hit.payload['type']

            is_ok, reason = check_relevance_judge_smart(llm, story, topic, negatives, doc_text, source)
            norm_score = min(max(hit.score, 0), 1) * 100

            icon = "âœ…" if is_ok else "âŒ"
            print(f"{icon} [{type_desc}] {source:<20} | GÃ¼ven: %{norm_score:.1f}")

            if is_ok:
                extra_context = ""
                # V44 Ã–zelliÄŸi: Smart Stitching
                if type_desc == "EMSAL KARAR":
                    real_path = os.path.join(SOURCES["emsal"]["folder"], source)
                    verdict = extract_pdf_conclusion(real_path)
                    # BurayÄ± daha belirgin yapÄ±yoruz
                    extra_context = f"\n\nğŸ›‘ BU BELGE BÄ°R MAHKEME KARARIDIR. Ä°ÅTE SONUCU ({source}):\n{verdict}\nğŸ›‘ KARAR SONU."

                valid_docs.append({
                    "source": source,
                    "page": page,
                    "type": type_desc,
                    "text": doc_text + extra_context,
                    "score": hit.score,
                    "reason": reason
                })

        if not valid_docs: print("ğŸ”´ YargÄ±Ã§ tÃ¼m belgeleri eledi."); continue

        context_str = ""
        for d in valid_docs:
            context_str += f"""
>>> TÃœR: [{d['type']}]
DOSYA ADI: {d['source']}
SAYFA: {d['page']}
NEDEN SEÃ‡Ä°LDÄ°: {d['reason']}
Ä°Ã‡ERÄ°K:
{d['text']}
=========================================
"""

        print("\nğŸ§‘â€âš–ï¸  AVUKAT YAZIYOR (Full Disclosure Mode)...")

        # --- V46 PROMPT GÃœNCELLEMESÄ° (HEPSÄ°NÄ° LÄ°STELE EMRÄ°) ---
        prompt = f"""
SEN KIDEMLÄ° BÄ°R HUKUKÃ‡USUN.
AÅŸaÄŸÄ±daki "DELÄ°LLER" kÄ±smÄ±ndaki metinleri kullanarak olayÄ± analiz et.

OLAY: "{story}"
ODAK: "{topic}"

DELÄ°LLER:
{context_str}

âš ï¸ KIRMIZI Ã‡Ä°ZGÄ°LER VE KURALLAR (BUNLARA UYMAZSAN Ä°ÅLEM GEÃ‡ERSÄ°ZDÄ°R):

1. **KAYNAK AYRIMI:** - [MEVZUAT] tÃ¼rÃ¼ndeki belgeleri SADECE "Mevzuat DayanaklarÄ±" baÅŸlÄ±ÄŸÄ±nda kullan.
   - [EMSAL KARAR] tÃ¼rÃ¼ndeki belgeleri SADECE "Ä°lgili Emsal Kararlar" baÅŸlÄ±ÄŸÄ±nda kullan.
   - ASLA bir Mevzuat belgesini (Ã¶rneÄŸin Miras Hukuku.pdf) Emsal Karar gibi sunma!

2. **FORMAT:**

   A. MEVZUAT DAYANAKLARI
      - Deliller listesindeki [MEVZUAT] etiketli belgelerin HEPSÄ°NÄ° tek tek maddeler halinde yaz. HiÃ§birini atlama.
      - Kanun maddelerini ve hukuki teoriyi buraya yaz.
      - Kaynak: (Dosya AdÄ±, Sayfa)

   B. Ä°LGÄ°LÄ° EMSAL KARARLAR (YargÄ±tay/Ä°stinaf)
      - Deliller listesindeki [EMSAL KARAR] etiketli belgelerin HEPSÄ°NÄ° tek tek Ã¶zetle. HiÃ§birini atlama.
      - Deliller listesinde [EMSAL KARAR] yazan belgeler var mÄ±? Varsa buraya yaz.
      - Yoksa "Ä°ncelenen belgeler arasÄ±nda doÄŸrudan bir emsal karar dosyasÄ± bulunamadÄ±" de. ASLA kitaplardan Ã¶rnek uydurma.
      - EÄŸer [EMSAL KARAR] varsa, metnin sonundaki "OTOMATÄ°K EKLENEN KARAR SONUCU" veya "HÃœKÃœM" kÄ±smÄ±nÄ± Ã¶zetle.
      - Kaynak: (Dosya AdÄ±, Sayfa)

   C. SONUÃ‡ VE TAVSÄ°YE
      - Net ve uygulanabilir bir yol haritasÄ±.

3. **ASLA "X DosyasÄ±" DEME:** DosyanÄ±n tam adÄ±nÄ± (Ã¶rn: buyuk7.pdf) kullan.

ANALÄ°ZÄ° BAÅLAT:
"""
        full_res = ""
        for chunk in llm.stream(prompt):
            c = chunk.content;
            full_res += c;
            print(c, end="", flush=True)
        print("\n")

        print("\n" + "=" * 20 + " ğŸ“š KULLANILAN KAYNAKLAR " + "=" * 20)
        for d in valid_docs:
            print(f"â€¢ [{d['type']}] {d['source']} (Sf. {d['page']})")
        print("=" * 64)

        create_pdf_report(story, valid_docs, full_res)


if __name__ == "__main__":
    main()